{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforce  Monte Carlo Algorithm\n",
    "In this notebook, you'll code The Reinforce (also called Monte Carlo Policy Gradient) algorithm  from scratch: .\n",
    "\n",
    "Reinforce is a *Policy-based method*: a Deep Reinforcement Learning algorithm that tries **to optimize the policy directly without using an action-value function**.\n",
    "\n",
    "More precisely, Reinforce is a *Policy-gradient method*, a subclass of *Policy-based methods* that aims **to optimize the policy directly by estimating the weights of the optimal policy using gradient ascent**.\n",
    "\n",
    "To test its robustness, we're going to train it in Cartpole-v1 environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import the packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gym\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check if we have a GPU\n",
    "- Let's check if we have a GPU `device:cuda0`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Agent: Playing CartPole-v1 ðŸ¤–\n",
    "### The CartPole-v1 environment\n",
    "\n",
    "> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    "\n",
    "So, we start with CartPole-v1. The goal is to push the cart left or right **so that the pole stays in the equilibrium.**\n",
    "\n",
    "The episode ends if:\n",
    "- The pole Angle is greater than Â±12Â°\n",
    "- Cart Position is greater than Â±2.4\n",
    "- Episode length is greater than 500\n",
    "\n",
    "We get a reward ðŸ’° of +1 every timestep the Pole stays in the equilibrium."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "# Create the env\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# Create the evaluation env\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "# Get the state space and action space\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "The State Space is:  4\n",
      "Sample observation [ 2.8096421e+00 -1.9910298e+38 -1.8927605e-01  4.2021248e+37]\n"
     ]
    }
   ],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "The Action Space is:  2\n",
      "Action Space Sample 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's build the Reinforce Architecture\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png\" alt=\"Reinforce\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So we want:\n",
    "- Two fully connected layers (fc1 and fc2).\n",
    "- Using ReLU as activation function of fc1\n",
    "- Using Softmax to output a probability distribution over actions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return #------add code here--------#\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = #------add code here--------#\n",
    "        m = Categorical(probs)\n",
    "        action = #------add code here--------#\n",
    "        return action.item(), m.log_prob(action)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Since **we want to sample an action from the probability distribution over actions (i.e., Stochastic policy)**, we can't use `action = np.argmax(m)` since it will always output the action that have the highest probability.\n",
    "\n",
    "- We need to replace with `action = m.sample()` that will sample an action from the probability distribution P(.|s)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's build the Reinforce Training Algorithm\n",
    "This is the Reinforce algorithm pseudocode:\n",
    "<img src=\"https://github.com/malkiAbdelhamid/Advanced-Deep-Learning-2023-2024-esisba/blob/master/lab3_Policy_Gradient/Reinforce_algo.png?raw=true\" alt=\"Policy gradient pseudocode\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building the parts of our algorithm #\n",
    "The main steps for building a Reinforce Algo are:\n",
    "1. Generates a trajectory (Line 3)\n",
    "2. Compute the discounted returns (Lines 4 & 5)\n",
    "3. Standardization of the returns\n",
    "4. Calculate current loss (objective function $J(\\theta)$) and estimate the gradient (Lines 6 & 7)\n",
    "5. Update parameters (Line 8)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Line 3 of pseudocode\n",
    "Based on the policy, the method **generate_trajectory(policy, max_t)** generates a trajectory and return two lists:\n",
    "  - saved_log_probs: the log probability of each action in the trajectory\n",
    "  - rewards: the list of the obtained rewards"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    " def generate_trajectory(policy, max_t):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = #add code here\n",
    "            # add the obtained probability to saved_log_probs list\n",
    "            # apply the action on the env\n",
    "            # add the obtained rewards to rewards list\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "\n",
    "        return  saved_log_probs, rewards"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### Lines 4 & 5 of pseudocode\n",
    " the method **computer_cumulative_reward(rewards,max_t,gamma)** Compute the discounted returns at each timestep, as the sum of the gamma-discounted return at time t (G_t) + the reward at time t"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "# Line 6 of pseudocode: calculate the return\n",
    "\n",
    "def computer_cumulative_reward(rewards,max_t,gamma):\n",
    "        ##we calculate the returns at timestep t as:\n",
    "        #           G_(t)= reward[t] + gamma * G_(t+1)\n",
    "        #\n",
    "        ## We compute this starting from the last timestep to the first, in order\n",
    "        ## to employ the formula presented above and avoid redundant computations that would be needed\n",
    "        ## if we were to do it from first to last.\n",
    "\n",
    "        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
    "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
    "        ## a normal python list would instead require O(N) to do this.\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft( #------add code here--------#)\n",
    "        return returns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### standardization of the returns\n",
    "- is employed to make training more stable\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "def returns_standardization(returns):\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        ## eps is the smallest representable float, which is\n",
    "        # added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        return returns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### Line 6, 7 & 8 of pseudocode\n",
    "**why do we minimize the loss**? You talked about Gradient Ascent not Gradient Descent?\n",
    "\n",
    "- We want to maximize our utility function $J(\\theta)$ but in PyTorch like in Tensorflow it's better to **minimize an objective function.**\n",
    "    - So let's say we want to reinforce action 3 at a certain timestep. Before training this action P is 0.25.\n",
    "    - So we want to modify $\\theta$ such that $\\pi_\\theta(a_3|s; \\theta) > 0.25$\n",
    "    - Because all P must sum to 1, max $\\pi_\\theta(a_3|s; \\theta)$ will **minimize other action probability.**\n",
    "    - So we should tell PyTorch **to min $1 - \\pi_\\theta(a_3|s; \\theta)$.**\n",
    "    - This loss function approaches 0 as $\\pi_\\theta(a_3|s; \\theta)$ nears 1.\n",
    "    - So we are encouraging the gradient to max $\\pi_\\theta(a_3|s; \\theta)$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "\n",
    "def compute_loss_and_train_policy(optimizer, saved_log_probs, returns):\n",
    "        # Line 6\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            #------add code here--------#\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # Line 7 & 8: PyTorch prefers gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge all functions into the reinforce method ##\n",
    "\n",
    "You will now see how the overall **Reinforce Algo** is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "\n",
    "        # Line 3 of pseudocode\n",
    "        saved_log_probs, rewards= #------add code here--------#\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # Lines 4 & 5 of pseudocode: calculate the return\n",
    "        returns=#------add code here--------#\n",
    "\n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        returns= #------add code here--------#\n",
    "\n",
    "        # Lines 6, 7 & 8\n",
    "        #------add code here--------#\n",
    "\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Train it\n",
    "- We're now ready to train our agent.\n",
    "- But first, we define a variable containing all the training hyperparameters.\n",
    "- You can change the training parameters (and should ðŸ˜‰)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    \"h_size\": 64,\n",
    "    \"n_training_episodes\": 1500,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
    "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 83.88\n",
      "Episode 200\tAverage Score: 334.38\n",
      "Episode 300\tAverage Score: 262.01\n",
      "Episode 400\tAverage Score: 451.66\n",
      "Episode 500\tAverage Score: 482.95\n",
      "Episode 600\tAverage Score: 484.05\n"
     ]
    }
   ],
   "source": [
    "scores = reinforce(cartpole_policy,\n",
    "                   cartpole_optimizer,\n",
    "                   cartpole_hyperparameters[\"n_training_episodes\"],\n",
    "                   cartpole_hyperparameters[\"max_t\"],\n",
    "                   cartpole_hyperparameters[\"gamma\"],\n",
    "                   100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores= pd.Series(scores, name=\"scores_REINFORCE\")\n",
    "scores.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "_ = scores.plot(ax=ax, label=\"scores_REINFORCE\")\n",
    "_ = (scores.rolling(window=100)\n",
    "           .mean()\n",
    "           .rename(\"Rolling Average\")\n",
    "           .plot(ax=ax))\n",
    "ax.legend()\n",
    "_ = ax.set_xlabel(\"Episode Number\")\n",
    "_ = ax.set_ylabel(\"scores_REINFORCE\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
