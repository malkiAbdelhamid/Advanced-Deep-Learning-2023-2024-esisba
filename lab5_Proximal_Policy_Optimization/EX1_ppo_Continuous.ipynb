{
 "cells": [
  {
   "cell_type": "raw",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from collections import namedtuple,deque\n",
    "from itertools import count\n",
    "\n",
    "import os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Categorical\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from tensorboardX import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "env_id = \"HalfCheetah-v4\"\n",
    "# Create the env\n",
    "env = gym3.make(env_id)\n",
    "\n",
    "# Create the evaluation env\n",
    "eval_env = gym3.make(env_id)\n",
    "\n",
    "# Get the state space and action space\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = #add code here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.action_head = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.action_head(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state,actions=None):\n",
    "\n",
    "        actions = self.forward(state).cpu()\n",
    "        m = #use a normal distribution with a standard deviation that equals to 0.1\n",
    "\n",
    "        if actions is None:\n",
    "           actions = #add code here\n",
    "\n",
    "        return actions.detach().numpy().squeeze(0), m.log_prob(actions)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "#Using a neural network to learn state value\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    #Takes in state\n",
    "    def __init__(self, s_size, h_size):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(s_size, h_size)\n",
    "        self.output_layer = nn.Linear(h_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #input layer\n",
    "        x = self.input_layer(x)\n",
    "        #activiation relu\n",
    "        x = F.relu(x)\n",
    "        #get state value\n",
    "        state_value = self.output_layer(x)\n",
    "\n",
    "        return state_value"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def generate_trajectory(actor, critic, max_t):\n",
    "\n",
    "        buffer = []\n",
    "        Experience=namedtuple('experience', ['state', 'action',  'old_log_prob', 'reward',  'next_state','state_val'])\n",
    "\n",
    "        state,_ = env.reset()\n",
    "        for t in range(max_t):\n",
    "            state_tensor=torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "            action, log_prob = #add code here\n",
    "\n",
    "            state_val=#add code here\n",
    "\n",
    "            next_state, reward,done, _,_ = env.step(action)\n",
    "\n",
    "            buffer.append( Experience(state, action, log_prob, reward, next_state,state_val))\n",
    "\n",
    "            state=next_state\n",
    "\n",
    "            if done:\n",
    "\n",
    "                break\n",
    "\n",
    "        return  buffer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def computer_cumulative_reward(rewards, max_t,gamma):\n",
    "\n",
    "        returns =deque(maxlen=len(rewards))\n",
    "\n",
    "        for t in range(len(rewards))[::-1]:\n",
    "          disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "          returns.appendleft( rewards[t]+gamma*disc_return_t)\n",
    "        return returns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def returns_standardization(returns):\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        ## eps is the smallest representable float, which is\n",
    "        # added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns,dtype=torch.float)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        return returns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "#['state', 'action',  'old_log_prob', 'reward',  'next_state','state_val']\n",
    "def train_actor_critic(actor, critic, actorOptimizer,criticOptimizer, buffer, returns, clip_param, writer,training_step):\n",
    "\n",
    "\n",
    "        states = [t.state for t in buffer]\n",
    "        actions = torch.tensor([t.action for t in buffer], dtype=torch.long).view(-1, 1)\n",
    "        old_log_probs = torch.stack([t.old_log_prob for t in buffer]).squeeze(1).detach()\n",
    "        state_values = torch.tensor([t.state_val for t in buffer], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "        #state_values= torch.stack([t.state_val for t in buffer]).squeeze()\n",
    "        actor_loss=[]\n",
    "        for  state, action,old_log_prob, Gt, state_value  in zip(states, actions,old_log_probs, returns, state_values):\n",
    "            state=torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            advantage=#add code here\n",
    "\n",
    "            _,new_log_prob=#add code here\n",
    "\n",
    "\n",
    "            ratio=#add code here\n",
    "\n",
    "            surr1 = #add code here\n",
    "            surr2 = #add code here: use torch.clamp()\n",
    "\n",
    "            # update actor network\n",
    "            actor_loss.append(-torch.min(surr1, surr2))\n",
    "\n",
    "\n",
    "\n",
    "        actor_loss = torch.cat(actor_loss).mean()\n",
    "\n",
    "        writer.add_scalar('loss/action_loss', actor_loss, global_step=training_step)\n",
    "        actorOptimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actorOptimizer.step()\n",
    "\n",
    "        #update critic network\n",
    "\n",
    "        value_loss = F.mse_loss(state_values, returns)\n",
    "        writer.add_scalar('loss/value_loss', value_loss, global_step=training_step)\n",
    "        criticOptimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        criticOptimizer.step()\n",
    "        training_step += 1\n",
    "\n",
    "        return training_step\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def PPO(actor,critic, actorOptimizer,criticOptimizer, n_training_episodes,  max_t, gamma,clip_param, writer, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    training_step=0\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "\n",
    "        # Generate an episode\n",
    "        buffer=generate_trajectory(actor,critic,max_t)\n",
    "        rewards = [t.reward for t in buffer]\n",
    "\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # calculate the return\n",
    "        returns= computer_cumulative_reward(rewards,max_t,gamma)\n",
    "\n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        returns=returns_standardization(returns)\n",
    "\n",
    "        # Train Actor and Critic networks\n",
    "\n",
    "        training_step=train_actor_critic(actor, critic, actorOptimizer,criticOptimizer, buffer, returns, clip_param, writer,training_step)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "HalfCheetah_hyperparameters = {\n",
    "    \"h_size\": 64,\n",
    "    \"n_training_episodes\": 300,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 500,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "    \"clip_param\":0.2\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Create actor and place it to the device\n",
    "HalfCheetah_actor = Actor(HalfCheetah_hyperparameters[\"state_space\"], HalfCheetah_hyperparameters[\"action_space\"], HalfCheetah_hyperparameters[\"h_size\"]).to(device)\n",
    "\n",
    "HalfCheetah_actorOptimizer = optim.Adam(HalfCheetah_actor.parameters(), lr=HalfCheetah_hyperparameters[\"lr\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Create critic and place it to the device\n",
    "HalfCheetah_critic = Critic(HalfCheetah_hyperparameters[\"state_space\"], HalfCheetah_hyperparameters[\"h_size\"]).to(device)\n",
    "HalfCheetah_criticOptimizer = optim.Adam(HalfCheetah_critic.parameters(), lr=HalfCheetah_hyperparameters[\"lr\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "run_name = f\"__{int(time.time())}\"\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: -8.86\n",
      "Episode 20\tAverage Score: -8.51\n",
      "Episode 30\tAverage Score: -8.68\n",
      "Episode 40\tAverage Score: -8.60\n",
      "Episode 50\tAverage Score: -8.62\n",
      "Episode 60\tAverage Score: -8.67\n",
      "Episode 70\tAverage Score: -8.66\n",
      "Episode 80\tAverage Score: -8.68\n",
      "Episode 90\tAverage Score: -8.66\n",
      "Episode 100\tAverage Score: -8.68\n",
      "Episode 110\tAverage Score: -8.62\n",
      "Episode 120\tAverage Score: -8.66\n",
      "Episode 130\tAverage Score: -8.64\n",
      "Episode 140\tAverage Score: -8.70\n",
      "Episode 150\tAverage Score: -8.72\n",
      "Episode 160\tAverage Score: -8.69\n",
      "Episode 170\tAverage Score: -8.73\n",
      "Episode 180\tAverage Score: -8.74\n",
      "Episode 190\tAverage Score: -8.81\n",
      "Episode 200\tAverage Score: -8.78\n",
      "Episode 210\tAverage Score: -8.80\n",
      "Episode 220\tAverage Score: -8.83\n",
      "Episode 230\tAverage Score: -8.81\n",
      "Episode 240\tAverage Score: -8.75\n",
      "Episode 250\tAverage Score: -8.76\n",
      "Episode 260\tAverage Score: -8.77\n",
      "Episode 270\tAverage Score: -8.75\n",
      "Episode 280\tAverage Score: -8.73\n",
      "Episode 290\tAverage Score: -8.72\n",
      "Episode 300\tAverage Score: -8.72\n"
     ]
    }
   ],
   "source": [
    "scores = PPO(HalfCheetah_actor, HalfCheetah_critic,\n",
    "                   HalfCheetah_actorOptimizer,HalfCheetah_criticOptimizer,\n",
    "                   HalfCheetah_hyperparameters[\"n_training_episodes\"],\n",
    "                   HalfCheetah_hyperparameters[\"max_t\"],\n",
    "                   HalfCheetah_hyperparameters[\"gamma\"],\n",
    "                   HalfCheetah_hyperparameters[\"clip_param\"],\n",
    "                   writer,\n",
    "                   10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
