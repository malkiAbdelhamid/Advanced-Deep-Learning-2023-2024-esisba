{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Actor-Critic Algorithm\n",
    "In this notebook, you'll code The Actor-Critic Algorithm from scratch: .\n",
    "\n",
    "Actor-Critic algorithm is a *Policy-based method* that aims to reduce the variance of the Reinforce algorithm and train our agent faster and better by using a combination of Policy-Based and Value-Based methods\n",
    "\n",
    "\n",
    "To test its robustness, we're going to train it in Cartpole-v1 environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import the packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gym\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check if we have a GPU\n",
    "- Let's check if we have a GPU `device:cuda0`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Agent: Playing CartPole-v1 ðŸ¤–\n",
    "### The CartPole-v1 environment\n",
    "\n",
    "> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    "\n",
    "So, we start with CartPole-v1. The goal is to push the cart left or right **so that the pole stays in the equilibrium.**\n",
    "\n",
    "The episode ends if:\n",
    "- The pole Angle is greater than Â±12Â°\n",
    "- Cart Position is greater than Â±2.4\n",
    "- Episode length is greater than 500\n",
    "\n",
    "We get a reward ðŸ’° of +1 every timestep the Pole stays in the equilibrium."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "# Create the env\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# Create the evaluation env\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "# Get the state space and action space\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's build the A2C algo\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Using a neural network to learn our actor (policy) parameters\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Using a neural network to learn state value\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    #Takes in state\n",
    "    def __init__(self, s_size, h_size):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # two fully connected layers\n",
    "        # add code here\n",
    "        # add code here\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #input layer\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        #activiation relu\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #get state value\n",
    "        state_value = self.output_layer(x)\n",
    "\n",
    "        return state_value\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building the parts of our algorithm #\n",
    "The main steps for building a A2C Algo are:\n",
    "1. Generates a trajectory\n",
    "2. Compute the discounted returns\n",
    "3. Standardization of the returns\n",
    "4. Train critic network\n",
    "5. Train actor network\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " def generate_trajectory(actor, critic, max_t):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state_values=[]\n",
    "\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            state=torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "            action, log_prob =  # add code here\n",
    "\n",
    "            # get the state value from th critic network\n",
    "            state_val= # add code here\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # add te obtained results to their relative lists ==> saved_log_probs, rewards, state_values\n",
    "\n",
    "            # add code here\n",
    "            # add code here\n",
    "            # add code here\n",
    "\n",
    "            state=next_state\n",
    "\n",
    "            if done:\n",
    "\n",
    "                break\n",
    "\n",
    "        return  saved_log_probs, rewards, state_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def computer_cumulative_reward(rewards, max_t,gamma):\n",
    "\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "        for t in range(n_steps)[::-1]:\n",
    "          disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "          returns.appendleft( rewards[t]+gamma*disc_return_t)\n",
    "        return returns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def returns_standardization(returns):\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        ## eps is the smallest representable float, which is\n",
    "        # added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        return returns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def train_actor(actorOptimizer,saved_log_probs, returns,state_values):\n",
    "\n",
    "\n",
    "        state_values= torch.stack(state_values).squeeze()\n",
    "\n",
    "        #calculate Advantage for actor\n",
    "        advantages = # add code here\n",
    "\n",
    "        #convect the advantages to a tensor\n",
    "        advantages = # add code here\n",
    "\n",
    "        actor_loss = []\n",
    "        # compute the actor loss\n",
    "        # add code here\n",
    "\n",
    "\n",
    "        actor_loss = torch.cat(actor_loss).sum()\n",
    "        # Backpropagate actor\n",
    "        actorOptimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actorOptimizer.step()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def train_critic(criticOptimizer, returns,state_values):\n",
    "        state_values= torch.stack(state_values).squeeze()\n",
    "\n",
    "        critic_loss=#add code here\n",
    "\n",
    "        # Backpropagate crtic\n",
    "        criticOptimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        criticOptimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge all functions into the Actor_Critic method ##\n",
    "\n",
    "You will now see how the overall **A2C Algo** is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def Actor_Critic(actor,critic, actorOptimizer,criticOptimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "\n",
    "        # Generate an episode\n",
    "        #add code here\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # calculate the return\n",
    "        returns= computer_cumulative_reward(rewards,max_t,gamma)\n",
    "\n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        returns=returns_standardization(returns)\n",
    "\n",
    "        # Train the Critic network\n",
    "        #add code here\n",
    "\n",
    "        # Train the Actor network\n",
    "        #add code here\n",
    "\n",
    "\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Train it\n",
    "- We're now ready to train our agent.\n",
    "- But first, we define a variable containing all the training hyperparameters.\n",
    "- You can change the training parameters (and should ðŸ˜‰)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    \"h_size\": 64,\n",
    "    \"n_training_episodes\": 1000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create actor and place it to the device\n",
    "cartpole_actor =#add code here\n",
    "\n",
    "cartpole_actorOptimizer = #add code here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create critic and place it to the device\n",
    "cartpole_critic = #add code here\n",
    "cartpole_criticOptimizer = #add code here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = Actor_Critic(cartpole_actor,\n",
    "                   cartpole_critic,cartpole_actorOptimizer,cartpole_criticOptimizer,\n",
    "                   cartpole_hyperparameters[\"n_training_episodes\"],\n",
    "                   cartpole_hyperparameters[\"max_t\"],\n",
    "                   cartpole_hyperparameters[\"gamma\"],\n",
    "                   100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores= pd.Series(scores, name=\"scores_Actor\")\n",
    "scores.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "_ = scores.plot(ax=ax, label=\"scores_Actor\")\n",
    "_ = (scores.rolling(window=100)\n",
    "           .mean()\n",
    "           .rename(\"Rolling Average\")\n",
    "           .plot(ax=ax))\n",
    "ax.legend()\n",
    "_ = ax.set_xlabel(\"Episode Number\")\n",
    "_ = ax.set_ylabel(\"scores_Actor\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
